{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRfLaIjDIvMQ"
   },
   "source": [
    "## **Ecommerce Product Categorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN7SlWd9I7aQ"
   },
   "source": [
    "**Problem Statement:** In the rapidly evolving world of eCommerce, accurate product categorization is crucial for ensuring seamless customer experiences, reducing search friction, and increasing product discoverability. However, the sheer volume of diverse products poses a significant challenge. Current classification systems struggle to handle ambiguities, unconventional naming conventions, and multi-language data. This hackathon aims to address these challenges by inviting participants to create innovative solutions that enhance product categorization efficiency, accuracy, and scalability.\n",
    "Develop a text classification model that categorizes products with maximum accuracy based on description of the product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIIKfuQ6zxg6"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**1: Import Libraries**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0rBcq7t525S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import WordPunctTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNXpChioZWsC"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Embedding, Conv1D, MaxPooling1D, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHhvjSK30FHI"
   },
   "source": [
    "## <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**2: Read dataset**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvrKOCB0S2vU"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/test_data.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX2c0FlmT9zX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/train_product_data.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlicUqhVRNJF"
   },
   "outputs": [],
   "source": [
    "df = df[['description','product_category_tree']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIw6GV4KRNt0"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oJZ9ry3Rrpn"
   },
   "outputs": [],
   "source": [
    "df['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MeLHmJimRr06"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkLkSnFO3zA6"
   },
   "outputs": [],
   "source": [
    "# Extract the main category from product_category_tree column by specifying regex pattern.\n",
    "# If not found, split the sentence and get the first token after removing extra spaces and chars.\n",
    "\n",
    "categories = df[\"product_category_tree\"].copy()\n",
    "\n",
    "for i in range(categories.shape[0]):\n",
    "  z = re.match(\"(.*?)>\",categories[i])  # splitting at '>'\n",
    "  # print(categories[i])\n",
    "  if z==None:\n",
    "    categories[i] = categories[i].split()[0].strip('[\"]')\n",
    "  else:\n",
    "    z = z.group().strip('[\"]>')         # removing special characters\n",
    "    categories[i]=z.strip()\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcwZSGl708c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's see top 40 unique categories with their frequencies\n",
    "\n",
    "print(categories.value_counts()[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHdfyA5H3Sf0"
   },
   "source": [
    "## <p id=\"3\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**3: Dataset Overview**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUsGy3E-S2xl"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yohHqhHk3zGl"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-E_FtJd-_D-"
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6liJsnllCm7_"
   },
   "outputs": [],
   "source": [
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkoY89StC1W9"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**4: EDA (Exploratory Data Analysis)**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kygUrDue10X"
   },
   "outputs": [],
   "source": [
    "# Explore data (descriptive statistics, missing values, etc.)\n",
    "print(df.describe())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyio6wao-_Kz"
   },
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y=\"product_category_tree\", data=df, order=df['product_category_tree'].value_counts().index)\n",
    "plt.title('Distribution of Product Categories')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lidPR3dZK0av"
   },
   "outputs": [],
   "source": [
    "# Step 3: Box Plot for Retail Price vs Category\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(y='product_category_tree', x='retail_price', data=df)\n",
    "plt.title('Retail Price by Product Category')\n",
    "plt.xlabel('Retail Price')\n",
    "plt.ylabel('Product Category')\n",
    "plt.xscale('log')  # Use log scale for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSbgNwcmLbN7"
   },
   "outputs": [],
   "source": [
    "# Step 6: Interactive Scatter Plot with Plotly\n",
    "fig = px.scatter(df, x='retail_price', y='discounted_price', color='product_category_tree',\n",
    "                 hover_data=['product_name'], title='Retail Price vs Discounted Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ynph4J15LxQv"
   },
   "outputs": [],
   "source": [
    "# Step 7: Animated Bar Plot of Category Distribution Over Time (if timestamp is available)\n",
    "if 'crawl_timestamp' in df.columns:\n",
    "    df['crawl_timestamp'] = pd.to_datetime(df['crawl_timestamp'])\n",
    "    df['year_month'] = df['crawl_timestamp'].dt.to_period('M')\n",
    "\n",
    "    category_month = df.groupby(['year_month', 'product_category_tree']).size().reset_index(name='counts')\n",
    "\n",
    "    fig = px.bar(category_month, x='product_category_tree', y='counts', color='product_category_tree',\n",
    "                 animation_frame='year_month', animation_group='product_category_tree', range_y=[0, df['product_category_tree'].value_counts().max()],\n",
    "                 title='Product Category Distribution Over Time')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCStmIreS7Ix"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**5: Text Normalization**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo7foG8b3zMw"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at all the unique category names\n",
    "# We will talk about their significance and validty later\n",
    "possible_labels = categories.unique()\n",
    "\n",
    "print(type(possible_labels))\n",
    "print(\"\\n\")\n",
    "print(\"Possible Labels: \\n\", possible_labels)\n",
    "print(\"\\n\")\n",
    "print(\"Number of possible categories:\", len(possible_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE7ux-FagXUx"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**5: Data Preprocessing**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb31QnHl2nzK"
   },
   "outputs": [],
   "source": [
    "# Step 4: Data Preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_SbUmYDVZ2U"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function to handle text data\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    return \"\"\n",
    "\n",
    "# Apply preprocessing to 'product_name' and 'description'\n",
    "df['product_name'] = df['product_name'].apply(preprocess_text)\n",
    "df['description'] = df['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cclealzlDiC8"
   },
   "outputs": [],
   "source": [
    "# Combine text features for better representation\n",
    "df['combined_text'] = df['product_name'] + ' ' + df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r38qfub8DiFI"
   },
   "outputs": [],
   "source": [
    "# Similar preprocessing for test data\n",
    "df['product_name'] = df['product_name'].apply(preprocess_text)\n",
    "df['description'] = df['description'].apply(preprocess_text)\n",
    "df['combined_text'] = df['product_name'] + ' ' + df['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNYktckxIfBS"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**6: Feature Engineering and Encoding**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A798dGXkDiIe"
   },
   "outputs": [],
   "source": [
    "# Step 5: Feature Engineering and Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['category'] = label_encoder.fit_transform(df['product_category_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsC4Z_zXDiOL"
   },
   "outputs": [],
   "source": [
    "# Vectorize text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df['combined_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df['combined_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9lMqVz8xCRO"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**7: Train-Test splite**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Gj-JqW3XEn0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWrHhjKuXEqq"
   },
   "outputs": [],
   "source": [
    "print(\"Testing data without label: \",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OQ20RuKXExX"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text = \" \".join(description for description in X_test)\n",
    "\n",
    "# Create the word cloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.title('Word Cloud for Normalized Descriptions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqguLUQPDiQm"
   },
   "outputs": [],
   "source": [
    "# Step 6: Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, df['category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ht2gWZ4xfr5"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**8: Model Building and Evaluation**</p>\n",
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw8oOBYsDiTI"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_val)\n",
    "print('Naive Bayes Accuracy:', accuracy_score(y_val, y_pred_nb))\n",
    "print('Naive Bayes F1 Score:', f1_score(y_val, y_pred_nb, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFfPPIrhxxtj"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOr-NhoXDiWC"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_val)\n",
    "print('Logistic Regression Accuracy:', accuracy_score(y_val, y_pred_lr))\n",
    "print('Logistic Regression F1 Score:', f1_score(y_val, y_pred_lr, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXqfG0gsyGAO"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NsYYB-A2n1n"
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_val)\n",
    "print('Support Vector Machine Accuracy:', accuracy_score(y_val, y_pred_svm))\n",
    "print('Support Vector Machine F1 Score:', f1_score(y_val, y_pred_svm, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8_bn1uAG17-"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBbXmU0EGy6P"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "print('Random Forest Accuracy:', accuracy_score(y_val, y_pred_rf))\n",
    "print('Random Forest F1 Score:', f1_score(y_val, y_pred_rf, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLkuLJ9dyMq7"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**9: Naive BayesDeep Learning Model**</p>\n",
    "## Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwq1I4AG2n4L"
   },
   "outputs": [],
   "source": [
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df['combined_text'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(df['combined_text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(df['combined_text'])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99_j5Vov2n8v"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_dl, X_val_dl, y_train_dl, y_val_dl = train_test_split(X_train_pad, df['category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rI7aW4Ntyzvl"
   },
   "source": [
    "##  Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O9oU8LJyooQ"
   },
   "outputs": [],
   "source": [
    "# Build LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=200))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_dl, y_train_dl, validation_data=(X_val_dl, y_val_dl), epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRi2zg6pyoq9"
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred_dl = np.argmax(model.predict(X_val_dl), axis=1)\n",
    "print('LSTM Accuracy:', accuracy_score(y_val_dl, y_pred_dl))\n",
    "print('LSTM F1 Score:', f1_score(y_val_dl, y_pred_dl, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZb21mCl3_FP"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**10: Hyperparameter Tuning**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xG_cRVMGyotc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example with Logistic Regression using GridSearchCV\n",
    "parameters = {'C': [0.1, 1, 10], 'penalty': ['l2']}\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), parameters, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "y_pred_best_lr = best_lr_model.predict(X_val)\n",
    "print('Tuned Logistic Regression Accuracy:', accuracy_score(y_val, y_pred_best_lr))\n",
    "print('Tuned Logistic Regression F1 Score:', f1_score(y_val, y_pred_best_lr, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAs0la1845xp"
   },
   "source": [
    "# <p id=\"1\" style=\"justify-content: center; align-items: center; background-color: #85C1E9; border-radius: 10px; border: 1px solid #3498DB; text-align: center; padding: 12px 0;\">**11: Final Evaluation**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G4UGC93yoyk"
   },
   "outputs": [],
   "source": [
    "# Use the best model to predict on the test set\n",
    "df_model = best_lr_model # or the chosen best model after tuning\n",
    "test_predictions = df_model.predict(X_test_tfidf)\n",
    "test_predictions_labels = label_encoder.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhZAZugtyo37"
   },
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "text_df = pd.DataFrame({'uniq_id': df['uniq_id'], 'predicted_category_tree': test_predictions_labels})\n",
    "text_df.to_csv('text_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1tekq3RDg6M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPCDvfZK2n_K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRJdiekp2oDx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpSdZeoI2oGH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
